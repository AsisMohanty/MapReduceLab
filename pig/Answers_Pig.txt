Ex 01 :: Word Count
-------------------

1.  Compare between Pig and Hadoop, including their pros and cons
  Pig is built on the top of Hadoop and it gives a more abstracted view on the algorithms implementation. The major advantage of Pig is that even complex algorithms can be simply implemented thanks to very expressive operators.
  Pig pros:
    - Simple language and debugging environment.
    - Customizable UDF.
    - Schema flexibility and low cost data curation (Pig eats anything!). 
    - Support for join.
  Pig cons:
    - Overhead while compiling Pig Latin into map-reduce jobs.
  MapReduce pros:
    - High customizable.
    - Java programming language.
    - Usually more efficient.
  MapReduce cons:
    - Rigid data-flow.
    - Strict model: one input, two stages, two operators.
    - Code is difficult to reuse and maintain even for basic operations.
    - Difficult algorithms design.

2.  What does a GROUP BY command do? At which phase of MapReduce is GROUP BY performed in this exercise and in general?
  PigLatin reference manual: "The GROUP BY operator groups together tuples that have the same group key. The result of a GROUP operation is a relation that includes one tuple per group." 

  Looking at the physical plan, we notice that the GROUP BY is actually replaced by three operators Local rearrange, Global rearrange and Package. Local Rearrange is the operator Pig uses to prepare data for the shuffle by setting up the key. Global Rearrange is a stand-in for the shuffle. Package sits in the reduce phase. Looking at the MapReduce plan, we can see that the Global rearrange operator disappeared because it is automatically performed by the Hadoop shuffle. However, the GROUP BY is always mapped in a MapReduce Job.

3.  What does a FOREACH command do? At which phase of MapReduce is FOREACH performed in this exercise and in general?
  The FOREACH command is used to act on every row in a relation, it can remove fields or generate new ones. In this case the first FOREACH is performed in the Map phase. On the contrary, as we can see from the MapReduce plan. the second FOREACH, which counts on the result of the GROUP BY operator, is performed in the Map phase, Combine phase and Reduce phase.  

4. Explain very briefly how Pig works (i.e. the process of Pig turning a Pig Latin script into runnable MapReduce job(s))
  Pig builds a logical plan for every bag, in this phase no processing is carried out. THe logical plan is compiled into a physical plan and then optimized. Finally, the physical plan is transformed into a chain of MapReduce jobs and executed in the cluster.

5. Explain how you come to a conclusion that your results are correct
  We executed the pig script on the file quote.txt of the first lab and we checked the STORE output against the one we obtained in the first lab.

Ex 02 :: Working with Online Social Networks data
------------------------------------------------

  Sub 01 :: Counting the number of "friends" per Twitter user
  -----------------------------------------------------------

  1. Are the output sorted? Why?
    
  The output were sorted by user ID because the underlying Hadoop framework automatically sorts by the key. In case of multiple reducers, each output file is sorted, but there is not a global sorted output.

  2. Are we able to pick the order: ascending or descending? How?

  We can use the relational operator ORDER BY and the DESC or ASC options. It is possible to order by any attribute of the tuple. 
  Example: sorted_friends = ORDER friends BY $0 DESC

  3. Related to job performance, what kinds of optimization does Pig provide in this exercise? Are they useful? Can we disable them? Should we?

  Pig used the default optimizations: combiner and multi-query optimization. A combiner is used in the GROUP BY + FOREACH job (if interleaved operations or nested foreach are not present) and the multi-query execution parse and optimize the whole script at once in order to combine intermediate tasks (for example the bad_dataset is never actually stored on the disk because it is not reused in any further operation).

  An additional optimization is called "parallelism", in other words the number of reducers that are instantiated for a job.
  The Pig framework estimates one reducer per Gigabyte, but it also possible to manually define it. 
    
  We can disable the multi-query execution doing: pig -no_multiquery myscript.pig, while we can not find any way to disable the combiner. However, they are usually very useful (as in this case), so they should be kept active.


  4. What should we do when the input has some noises? for example: some lines in the dataset only contain USER_ID but the FOLLOWER_ID is unavailable or null

  As we already did, the SPLIT operator can be used to filter out the noisy records.
  SPLIT dataset INTO good_dataset IF id is not null and fr is not null, bad_dataset OTHERWISE;

  Sub 02 :: Find the number of two-hop paths in the Twitter network
  -----------------------------------------------------------------
    
  1.  What is the size of the input data? In your opinion, is it considered Big Data? Why? How long does the job run? What is your comment here?
    Being the size of the input in the order of the MBs, we can not consider it Big Data (we expect data to be at least of the order of GBs).
    However, being a join involved in the algorithm, the intermediate results size can greatly increase. In fact, with 40 reducers, our job lasted circa 7 minutes. This result seems reasonable, having the script performed a join.

  2.  Try the parallelism with different numbers. Observe the behavior of reducers. What can you say about the load balancing between reducers?

    We tried to execute the pig script one time with 20 reducers and the second one with 40 reducers. We noticed that the load was not uniformly distributed among all the reducers, in fact the time delay between the fastest and the slowest reducer is circa 1 minute (on 2:40 minutes in average). However the execution with 40 reducer was a little faster but not enough to justify a double number of reducers.

  3.  Explain briefly how Pig JOIN works in your opinion.

    Pig gives the possibility to perform two type of join: the general join and the fragment replicate join. 
     - The general join is used for relations having about the same size. We suppose that Pig implements a reduce-side join in similar way we did in the MapReduce laboratory for the join exercise.
     - The fragment replicated join is used if one of the relations is small enough to fit in memory (the clause USING 'replicated' need to be used). In this case Pig performs a map-side join using an in-memory lookup table. We suppose that this solution is really similar to the Distributed Cache join done in the MapReduce lab.
    
  4.  Have you verified your results? Does your result contain duplicate tuple? Or any tuple that points from one user to him again? What operations do you use to solve these two problems?
  
    Yes, of course. In order to remove the duplicate tuples we used the DISTINCT operator, which is mapped by Pig on a MapReduce job. In order to remove the loops we used the FILTER statement: 

    result = FILTER d_result BY $0!=$1;

  5.  How many MapReduce jobs does your Pig script generate? Explain why

    Looking at the exec plan we can identify four different MapReduce jobs. 

    The first two jobs lack the reduce phase and are represented as executed in parallel by Hadoop. Actually, these two jobs map the load operation that is done two times on the same file in order to have the two relations used in the self join. As it is possible to notice in the physical plan, the framework physically performs the load operation two times without optimizations. The third job represents the JOIN, the FOREACH and the FILTER operations. The forth one the DISTINCT operation.

Exercise 3:: Working with Network Data
--------------------------------------
  
  Sub 01 :: Working on tcpdump data
  ---------------------------------

  The data preparation happens during the map phase of the Hadoop job where the input file is read. The REGEX_EXTRACT_ALL statement corresponds to a Pig built-in UDF function that receives as input a tuple and a regular expression and extracts a tuple of matched strings as output. Each group () in the regex corresponds to an attribute of the tuple (in this case: 3 integers as date, 4 groups of digits as source address, other 4 groups of digits as destination address and the integer after the word "length" as last attribute).
  In Pig, regular expressions use the Java syntax, due to the fact that everything is mapped into Java classes.
  
  Sub 02 :: Working on Network data
  ---------------------------------
  
  Starting from the two scripts provided, we figured out how to implement a Pig equivalent of the extended SQL roll-up operation (it is composed by three chained jobs).
  We have already showed the solution to the Pig laboratory assistant and he seemed quite impressed, so we supposed that is OK :)
  However, to improve the two scripts provided, we thought about:
  - Joining USING 'replicated' if the tables involved in the joins are supposed to be small enough to fit in memory. This feature implements a map side join thanks to the distributed cache system (as we saw in Ex 05 of the MapReduce Lab).
  - Setting the number of the reducers using PARALLEL.
  - We noticed that the three jobs (hour, day and week) are independent from each other in the way that are actually implemented in the solutions provided. Therefore, we suppose that, having previously generated the hour, day and week values from the original file, the three jobs may be executed in parallel, instead of one after the other.
